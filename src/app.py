import streamlit as st
import joblib
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from pyvi import ViTokenizer
import numpy as np
import pandas as pd
import io

# C·∫•u h√¨nh CSS t√πy ch·ªânh cho giao di·ªán
custom_css = """
<style>
    body {
        background-color: #0e0e0e;
        color: #FFFFFF;
    }
    .title {
        color: #2ee65f;
        font-size: 2.5em;
        font-weight: bold;
        text-align: center;
        margin-bottom: 20px;
    }
    .description {
        font-size: 1.2em;
        text-align: center;
        color: #ffffff;
        margin-bottom: 30px;
    }
    textarea, .stTextArea textarea {
        background-color: #333333;
        color: #FFFFFF;
        border: 1px solid #555555;
    }
    .predict-button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
        padding: 12px 24px;
        border: none;
        border-radius: 8px;
    }
    .footer a {
        color: #4CAF50;
        text-decoration: none;
    }
    .footer a:hover {
        text-decoration: underline;
    }
    .history-table {
        background-color: #1e1e1e;
        color: #FFFFFF;
        padding: 10px;
        border-radius: 8px;
    }
</style> 
"""

# √Åp d·ª•ng CSS t√πy ch·ªânh
st.markdown(custom_css, unsafe_allow_html=True)

# T·∫£i m√¥ h√¨nh v√† c√°c b·ªô m√£ h√≥a v·ªõi cache_resource
@st.cache_resource
def load_resources():
    import os
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_dir = os.path.join(base_dir, "data", "tokenizer_data")
    
    model = load_model(os.path.join(model_dir, "trained_model.keras"))
    vocab = joblib.load(os.path.join(model_dir, "vocab.pkl"))
    label_encoder_disease = joblib.load(os.path.join(model_dir, "label_encoder_disease.pkl"))
    label_encoder_prescription = joblib.load(os.path.join(model_dir, "label_encoder_prescription.pkl"))
    return model, vocab, label_encoder_disease, label_encoder_prescription

model, vocab, label_encoder_disease, label_encoder_prescription = load_resources()

def make_prediction(input_text, model, vocab, label_encoder_disease, label_encoder_prescription, max_len=100):
    if not input_text:
        return None, None
    
    tokenized_text = ViTokenizer.tokenize(input_text)
    sequence = []
    for word in tokenized_text.split():
        sequence.append(vocab.get(word, 0))  
    padded_sequence = pad_sequences([sequence], maxlen=max_len)
    
    predictions = model.predict(padded_sequence)
    disease_pred = np.argmax(predictions[0], axis=1)
    prescription_pred = np.argmax(predictions[1], axis=1)
    
    disease = label_encoder_disease.inverse_transform(disease_pred)[0]
    prescription = label_encoder_prescription.inverse_transform(prescription_pred)[0]
    
    return disease, prescription

# Kh·ªüi t·∫°o l·ªãch s·ª≠ d·ª± ƒëo√°n trong session_state
if 'history' not in st.session_state:
    st.session_state.history = []

# Streamlit App Layout
st.markdown('<div class="main-container">', unsafe_allow_html=True)
st.markdown('<div class="title">ü©∫ D·ª± ƒêo√°n B·ªánh v√† ƒê·ªÅ Xu·∫•t Thu·ªëc</div>', unsafe_allow_html=True)
st.markdown('<div class="description">M√¥ t·∫£ tri·ªáu ch·ª©ng c·ªßa b·ªánh nh√¢n ƒë·ªÉ nh·∫≠n d·ª± ƒëo√°n b·ªánh v√† ƒë·ªÅ xu·∫•t thu·ªëc.</div>', unsafe_allow_html=True)

# Khu v·ª±c nh·∫≠p li·ªáu v√† n√∫t d·ª± ƒëo√°n
# Lo·∫°i b·ªè b·ªë c·ª•c c·ªôt v√† s·∫Øp x·∫øp theo chi·ªÅu d·ªçc
# Text area cho ƒë·∫ßu v√†o
input_text = st.text_area("Nh·∫≠p tri·ªáu ch·ª©ng c·ªßa b·ªánh nh√¢n:", height=100)

if st.button("üîç D·ª± ƒêo√°n", key="predict_button", help="Nh·∫•n ƒë·ªÉ d·ª± ƒëo√°n d·ª±a tr√™n tri·ªáu ch·ª©ng ƒë√£ nh·∫≠p"):
    if input_text.strip():
        disease, prescription = make_prediction(input_text, model, vocab, label_encoder_disease, label_encoder_prescription)
        if disease and prescription:
            st.success(f"### üßæ B·ªánh D·ª± ƒêo√°n: **{disease}**")
            st.success(f"### üíä Thu·ªëc ƒê·ªÅ Xu·∫•t: **{prescription}**")
            
            # L∆∞u l·ªãch s·ª≠ d·ª± ƒëo√°n
            st.session_state.history.append({
                'Tri·ªáu ch·ª©ng nh·∫≠p v√†o': input_text,
                'B·ªánh d·ª± ƒëo√°n': disease,
                'Thu·ªëc ƒë·ªÅ xu·∫•t': prescription
            })
        else:
            st.error("D·ª± ƒëo√°n th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·∫ßu v√†o.")
    else:
        st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ tri·ªáu ch·ª©ng ƒë·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n.")

# Hi·ªÉn th·ªã l·ªãch s·ª≠ d·ª± ƒëo√°n b√™n d∆∞·ªõi
st.markdown("### üìú L·ªãch S·ª≠ D·ª± ƒêo√°n")
if st.session_state.history:
    history_df = pd.DataFrame(st.session_state.history)
    st.table(history_df)
    
    excel_buffer = io.BytesIO()
    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
        history_df.to_excel(writer, index=False, sheet_name='LichSuDuDoan')
    
    excel_buffer.seek(0)

    st.download_button(
        label="üì• T·∫£i Xu·ªëng L·ªãch S·ª≠ D·ª± ƒêo√°n",
        data=excel_buffer,
        file_name='lich_su_du_doan.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    )
else:
    st.info("Ch∆∞a c√≥ l·ªãch s·ª≠ d·ª± ƒëo√°n n√†o.")

st.markdown('</div>', unsafe_allow_html=True)

# Th√¥ng tin Sidebar
st.sidebar.title("üìã Gi·ªõi Thi·ªáu Website")
st.sidebar.markdown(
    """
    <div style="text-align: justify;">
        Website n√†y s·ª≠ d·ª•ng m√¥ h√¨nh h·ªçc m√°y ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n b·ªánh v√† ƒë·ªÅ xu·∫•t thu·ªëc d·ª±a tr√™n tri·ªáu ch·ª©ng.<br>
        Nh·∫≠p m√¥ t·∫£ tri·ªáu ch·ª©ng v√†o khu v·ª±c ch√≠nh v√† nh·∫•n<br>
        <strong>üîç D·ª± ƒêo√°n</strong> ƒë·ªÉ nh·∫≠n k·∫øt qu·∫£.
        <br>
    </div>
    """,
    unsafe_allow_html=True
)
st.sidebar.markdown(
    """
    ---
    <div style="text-align: justify;">
        <strong>üë®‚Äç‚öïÔ∏è L∆ØU √ù</strong><br>
        C√¥ng c·ª• n√†y d√†nh cho m·ª•c ƒë√≠ch gi√°o d·ª•c v√† ch∆∞a th·ªÉ thay th·∫ø t∆∞ v·∫•n y t·∫ø chuy√™n nghi·ªáp.
    </div>
    """,
    unsafe_allow_html=True)